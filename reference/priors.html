<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><!-- upstream: inst/BS5/templates/head.html; pkgdown-version: 2.1.3, fe04924 --><!-- https://github.com/r-lib/pkgdown/tree/fe04924b3df129bea60ac871614b01d87bcae147 --><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Prior distributions and options — priors • rstanarm</title><!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png"><link rel="icon" type="”image/svg+xml”" href="../favicon.svg"><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png"><link rel="icon" sizes="any" href="../favicon.ico"><link rel="manifest" href="../site.webmanifest"><!-- mathjax math --><script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0/tex-mml-chtml.js" integrity="sha256-qoRlVrS5NAnXSSSiMfFXwK8C9obG11Iybe4h2+bQYR4=" crossorigin="anonymous">
  </script><script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Source_Sans_3-0.4.10/font.css" rel="stylesheet"><link href="../deps/Source_Code_Pro-0.4.10/font.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- Font Awesome 7.0.1 for Bluesky icon --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css" referrerpolicy="no-referrer"><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Prior distributions and options — priors"><meta name="description" content="The functions described on this page are used to specify the
  prior-related arguments of the various modeling functions in the
  rstanarm package (to view the priors used for an existing model see
  prior_summary).
The default priors used in the various rstanarm modeling functions
  are intended to be weakly informative in that they provide moderate
  regularization and help stabilize computation. For many applications the
  defaults will perform well, but prudent use of more informative priors is
  encouraged. Uniform prior distributions are possible (e.g. by setting
  stan_glm's prior argument to NULL) but, unless
  the data is very strong, they are not recommended and are not
  non-informative, giving the same probability mass to implausible values as
  plausible ones.
More information on priors is available in the vignette
  Prior
  Distributions for rstanarm Models as well as the vignettes for the
  various modeling functions. For details on the
  priors used for multilevel models in particular see the vignette
  Estimating
  Generalized (Non-)Linear Models with Group-Specific Terms with rstanarm
  and also the Covariance matrices section lower down on this page."><meta property="og:description" content="The functions described on this page are used to specify the
  prior-related arguments of the various modeling functions in the
  rstanarm package (to view the priors used for an existing model see
  prior_summary).
The default priors used in the various rstanarm modeling functions
  are intended to be weakly informative in that they provide moderate
  regularization and help stabilize computation. For many applications the
  defaults will perform well, but prudent use of more informative priors is
  encouraged. Uniform prior distributions are possible (e.g. by setting
  stan_glm's prior argument to NULL) but, unless
  the data is very strong, they are not recommended and are not
  non-informative, giving the same probability mass to implausible values as
  plausible ones.
More information on priors is available in the vignette
  Prior
  Distributions for rstanarm Models as well as the vignettes for the
  various modeling functions. For details on the
  priors used for multilevel models in particular see the vignette
  Estimating
  Generalized (Non-)Linear Models with Group-Specific Terms with rstanarm
  and also the Covariance matrices section lower down on this page."><meta property="og:image" content="https://mc-stan.org/rstanarm/logo.svg"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <!-- upstream: inst/BS5/templates/navbar.html; pkgdown-version: 2.1.3, fe04924 -->
<!-- https://github.com/r-lib/pkgdown/tree/fe04924b3df129bea60ac871614b01d87bcae147 -->
<nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">


    <a class="navbar-brand me-2" href="../index.html">
      <!-- Add Stan logo -->
      <picture><source type="image/svg+xml" srcset="../logo.svg"><img src="../logo.png" class="stan-logo" alt="Stan blue hex logo"></source></picture>
      rstanarm
    </a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">2.32.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../index.html" aria-label="Go to homepage"><span class="fa fa-home fa-lg"></span></a></li>
<li class="nav-item"><a class="nav-link" href="../articles/index.html">Vignettes</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Functions</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-other-packages" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Other Packages</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-other-packages"><li><a class="external-link dropdown-item" href="https://mc-stan.org/bayesplot">bayesplot</a></li>
    <li><a class="external-link dropdown-item" href="https://mc-stan.org/cmdstanr">cmdstanr</a></li>
    <li><a class="external-link dropdown-item" href="https://mc-stan.org/loo">loo</a></li>
    <li><a class="external-link dropdown-item" href="https://mc-stan.org/posterior">posterior</a></li>
    <li><a class="external-link dropdown-item" href="https://mc-stan.org/projpred">projpred</a></li>
    <li><a class="external-link dropdown-item" href="https://mc-stan.org/rstan">rstan</a></li>
    <li><a class="external-link dropdown-item" href="https://mc-stan.org/rstantools">rstantools</a></li>
    <li><a class="external-link dropdown-item" href="https://mc-stan.org/shinystan">shinystan</a></li>
  </ul></li>
<li class="nav-item"><a class="external-link nav-link" href="https://mc-stan.org/about/">About Stan</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://bsky.app/profile/did:plc:qznndgdnkem2yryu7ipqbpv7" aria-label="Visit our Bluesky profile"><span class="fa fa-brands fa-bluesky"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://discourse.mc-stan.org/" aria-label="Visit our forums"><span class="fa fa-users"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/stan-dev/rstanarm/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch"><li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>Prior distributions and options</h1>
      <small class="dont-index">Source: <a href="https://github.com/stan-dev/rstanarm/blob/New-pkgdown-theme/R/priors.R" class="external-link"><code>R/priors.R</code></a></small>
      <div class="d-none name"><code>priors.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>The functions described on this page are used to specify the
  prior-related arguments of the various modeling functions in the
  <span class="pkg">rstanarm</span> package (to view the priors used for an existing model see
  <code><a href="prior_summary.stanreg.html">prior_summary</a></code>).</p>
<p>The default priors used in the various <span class="pkg">rstanarm</span> modeling functions
  are intended to be <em>weakly informative</em> in that they provide moderate
  regularization and help stabilize computation. For many applications the
  defaults will perform well, but prudent use of more informative priors is
  encouraged. Uniform prior distributions are possible (e.g. by setting
  <code><a href="stan_glm.html">stan_glm</a></code>'s <code>prior</code> argument to <code>NULL</code>) but, unless
  the data is very strong, they are not recommended and are <em>not</em>
  non-informative, giving the same probability mass to implausible values as
  plausible ones.</p>
<p>More information on priors is available in the vignette
  <a href="https://mc-stan.org/rstanarm/articles/priors.html"><em>Prior
  Distributions for rstanarm Models</em></a> as well as the vignettes for the
  various modeling functions. For details on the
  priors used for multilevel models in particular see the vignette
  <a href="https://mc-stan.org/rstanarm/articles/glmer.html"><em>Estimating
  Generalized (Non-)Linear Models with Group-Specific Terms with rstanarm</em></a>
  and also the <strong>Covariance matrices</strong> section lower down on this page.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">normal</span><span class="op">(</span>location <span class="op">=</span> <span class="fl">0</span>, scale <span class="op">=</span> <span class="cn">NULL</span>, autoscale <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">student_t</span><span class="op">(</span>df <span class="op">=</span> <span class="fl">1</span>, location <span class="op">=</span> <span class="fl">0</span>, scale <span class="op">=</span> <span class="cn">NULL</span>, autoscale <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">cauchy</span><span class="op">(</span>location <span class="op">=</span> <span class="fl">0</span>, scale <span class="op">=</span> <span class="cn">NULL</span>, autoscale <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">hs</span><span class="op">(</span>df <span class="op">=</span> <span class="fl">1</span>, global_df <span class="op">=</span> <span class="fl">1</span>, global_scale <span class="op">=</span> <span class="fl">0.01</span>, slab_df <span class="op">=</span> <span class="fl">4</span>, slab_scale <span class="op">=</span> <span class="fl">2.5</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">hs_plus</span><span class="op">(</span></span>
<span>  df1 <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  df2 <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  global_df <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  global_scale <span class="op">=</span> <span class="fl">0.01</span>,</span>
<span>  slab_df <span class="op">=</span> <span class="fl">4</span>,</span>
<span>  slab_scale <span class="op">=</span> <span class="fl">2.5</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu">laplace</span><span class="op">(</span>location <span class="op">=</span> <span class="fl">0</span>, scale <span class="op">=</span> <span class="cn">NULL</span>, autoscale <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">lasso</span><span class="op">(</span>df <span class="op">=</span> <span class="fl">1</span>, location <span class="op">=</span> <span class="fl">0</span>, scale <span class="op">=</span> <span class="cn">NULL</span>, autoscale <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">product_normal</span><span class="op">(</span>df <span class="op">=</span> <span class="fl">2</span>, location <span class="op">=</span> <span class="fl">0</span>, scale <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">exponential</span><span class="op">(</span>rate <span class="op">=</span> <span class="fl">1</span>, autoscale <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">decov</span><span class="op">(</span>regularization <span class="op">=</span> <span class="fl">1</span>, concentration <span class="op">=</span> <span class="fl">1</span>, shape <span class="op">=</span> <span class="fl">1</span>, scale <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">lkj</span><span class="op">(</span>regularization <span class="op">=</span> <span class="fl">1</span>, scale <span class="op">=</span> <span class="fl">10</span>, df <span class="op">=</span> <span class="fl">1</span>, autoscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">dirichlet</span><span class="op">(</span>concentration <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">R2</span><span class="op">(</span>location <span class="op">=</span> <span class="cn">NULL</span>, what <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"mode"</span>, <span class="st">"mean"</span>, <span class="st">"median"</span>, <span class="st">"log"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">default_prior_intercept</span><span class="op">(</span><span class="va">family</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">default_prior_coef</span><span class="op">(</span><span class="va">family</span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-location">location<a class="anchor" aria-label="anchor" href="#arg-location"></a></dt>
<dd><p>Prior location. In most cases, this is the prior mean, but
for <code>cauchy</code> (which is equivalent to <code>student_t</code> with
<code>df=1</code>), the mean does not exist and <code>location</code> is the prior
median. The default value is \(0\), except for <code>R2</code> which has no
default value for <code>location</code>. For <code>R2</code>, <code>location</code> pertains
to the prior location of the \(R^2\) under a Beta distribution, but the
interpretation of the <code>location</code> parameter depends on the specified
value of the <code>what</code> argument (see the <em>R2 family</em> section in
<strong>Details</strong>).</p></dd>


<dt id="arg-scale">scale<a class="anchor" aria-label="anchor" href="#arg-scale"></a></dt>
<dd><p>Prior scale. The default depends on the family (see
<strong>Details</strong>).</p></dd>


<dt id="arg-autoscale">autoscale<a class="anchor" aria-label="anchor" href="#arg-autoscale"></a></dt>
<dd><p>If <code>TRUE</code> then the scales of the priors on the
intercept and regression coefficients may be additionally modified
internally by <span class="pkg">rstanarm</span> in the following cases. First, for Gaussian
models only, the prior scales for the intercept, coefficients, and the
auxiliary parameter <code>sigma</code> (error standard deviation) are multiplied
by <code>sd(y)</code>. Additionally — not only for Gaussian models — if the
<code>QR</code> argument to the model fitting function (e.g. <code>stan_glm</code>) is
<code>FALSE</code> then we also divide the prior scale(s) by <code>sd(x)</code>.
Prior autoscaling is also discussed in the vignette
<a href="https://mc-stan.org/rstanarm/articles/priors.html"><em>Prior
Distributions for rstanarm Models</em></a></p></dd>


<dt id="arg-df-df-df-">df, df1, df2<a class="anchor" aria-label="anchor" href="#arg-df-df-df-"></a></dt>
<dd><p>Prior degrees of freedom. The default is \(1\) for
<code>student_t</code>, in which case it is equivalent to <code>cauchy</code>. For the
hierarchical shrinkage priors (<code>hs</code> and <code>hs_plus</code>) the degrees of
freedom parameter(s) default to \(1\). For the <code>product_normal</code>
prior, the degrees of freedom parameter must be an integer (vector) that is
at least \(2\) (the default).</p></dd>


<dt id="arg-global-df-global-scale-slab-df-slab-scale">global_df, global_scale, slab_df, slab_scale<a class="anchor" aria-label="anchor" href="#arg-global-df-global-scale-slab-df-slab-scale"></a></dt>
<dd><p>Optional arguments for the
hierarchical shrinkage priors. See the <em>Hierarchical shrinkage family</em>
section below.</p></dd>


<dt id="arg-rate">rate<a class="anchor" aria-label="anchor" href="#arg-rate"></a></dt>
<dd><p>Prior rate for the exponential distribution. Defaults to
<code>1</code>. For the exponential distribution, the rate parameter is the
<em>reciprocal</em> of the mean.</p></dd>


<dt id="arg-regularization">regularization<a class="anchor" aria-label="anchor" href="#arg-regularization"></a></dt>
<dd><p>Exponent for an LKJ prior on the correlation matrix in
the <code>decov</code> or <code>lkj</code> prior. The default is \(1\), implying a
joint uniform prior.</p></dd>


<dt id="arg-concentration">concentration<a class="anchor" aria-label="anchor" href="#arg-concentration"></a></dt>
<dd><p>Concentration parameter for a symmetric Dirichlet
distribution. The default is \(1\), implying a joint uniform prior.</p></dd>


<dt id="arg-shape">shape<a class="anchor" aria-label="anchor" href="#arg-shape"></a></dt>
<dd><p>Shape parameter for a gamma prior on the scale parameter in the
<code>decov</code> prior. If <code>shape</code> and <code>scale</code> are both \(1\) (the
default) then the gamma prior simplifies to the unit-exponential
distribution.</p></dd>


<dt id="arg-what">what<a class="anchor" aria-label="anchor" href="#arg-what"></a></dt>
<dd><p>A character string among <code>'mode'</code> (the default),
<code>'mean'</code>, <code>'median'</code>, or <code>'log'</code> indicating how the
<code>location</code> parameter is interpreted in the <code>LKJ</code> case. If
<code>'log'</code>, then <code>location</code> is interpreted as the expected
logarithm of the \(R^2\) under a Beta distribution. Otherwise,
<code>location</code> is interpreted as the <code>what</code> of the \(R^2\)
under a Beta distribution. If the number of predictors is less than
or equal to two, the mode of this Beta distribution does not exist
and an error will prompt the user to specify another choice for
<code>what</code>.</p></dd>


<dt id="arg-family">family<a class="anchor" aria-label="anchor" href="#arg-family"></a></dt>
<dd><p>Not currently used.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A named list to be used internally by the <span class="pkg">rstanarm</span> model
  fitting functions.</p>
    </div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>The details depend on the family of the prior being used:</p><div class="section">
<h3 id="student-t-family">Student t family<a class="anchor" aria-label="anchor" href="#student-t-family"></a></h3>
<p>Family members:</p><ul><li><p><code>normal(location, scale)</code></p></li>
<li><p><code>student_t(df, location, scale)</code></p></li>
<li><p><code>cauchy(location, scale)</code></p></li>
</ul><p>Each of these functions also takes an argument <code>autoscale</code>.</p>
<p>For the prior distribution for the intercept, <code>location</code>,
  <code>scale</code>, and <code>df</code> should be scalars. For the prior for the other
  coefficients they can either be vectors of length equal to the number of
  coefficients (not including the intercept), or they can be scalars, in
  which case they will be recycled to the appropriate length. As the
  degrees of freedom approaches infinity, the Student t distribution
  approaches the normal distribution and if the degrees of freedom are one,
  then the Student t distribution is the Cauchy distribution.</p>
<p>If <code>scale</code> is not specified it will default to \(2.5\), unless the
  probit link function is used, in which case these defaults are scaled by a
  factor of <code>dnorm(0)/dlogis(0)</code>, which is roughly \(1.6\).</p>
<p>If the <code>autoscale</code> argument is <code>TRUE</code>, then the
  scales will be further adjusted as described above in the documentation of
  the <code>autoscale</code> argument in the <strong>Arguments</strong> section.</p>
</div>
<div class="section">
<h3 id="hierarchical-shrinkage-family">Hierarchical shrinkage family<a class="anchor" aria-label="anchor" href="#hierarchical-shrinkage-family"></a></h3>
<p>Family members:</p><ul><li><p><code>hs(df, global_df, global_scale, slab_df, slab_scale)</code></p></li>
<li><p><code>hs_plus(df1, df2, global_df, global_scale, slab_df, slab_scale)</code></p></li>
</ul><p>The hierarchical shrinkage priors are normal with a mean of zero and a
  standard deviation that is also a random variable. The traditional
  hierarchical shrinkage prior utilizes a standard deviation that is
  distributed half Cauchy with a median of zero and a scale parameter that is
  also half Cauchy. This is called the "horseshoe prior". The hierarchical
  shrinkage (<code>hs</code>) prior in the <span class="pkg">rstanarm</span> package instead utilizes
  a regularized horseshoe prior, as described by Piironen and Vehtari (2017),
  which recommends setting the <code>global_scale</code> argument equal to the ratio
  of the expected number of non-zero coefficients to the expected number of
  zero coefficients, divided by the square root of the number of observations.</p>
<p>The hierarhical shrinkpage plus (<code>hs_plus</code>) prior is similar except
  that the standard deviation that is distributed as the product of two
  independent half Cauchy parameters that are each scaled in a similar way
  to the <code>hs</code> prior.</p>
<p>The hierarchical shrinkage priors have very tall modes and very fat tails.
  Consequently, they tend to produce posterior distributions that are very
  concentrated near zero, unless the predictor has a strong influence on the
  outcome, in which case the prior has little influence. Hierarchical
  shrinkage priors often require you to increase the
  <code><a href="adapt_delta.html">adapt_delta</a></code> tuning parameter in order to diminish the number
  of divergent transitions. For more details on tuning parameters and
  divergent transitions see the Troubleshooting section of the <em>How to
  Use the rstanarm Package</em> vignette.</p>
</div>
<div class="section">
<h3 id="laplace-family">Laplace family<a class="anchor" aria-label="anchor" href="#laplace-family"></a></h3>
<p>Family members:</p><ul><li><p><code>laplace(location, scale)</code></p></li>
<li><p><code>lasso(df, location, scale)</code></p></li>
</ul><p>Each of these functions also takes an argument <code>autoscale</code>.</p>
<p>The Laplace distribution is also known as the double-exponential
  distribution. It is a symmetric distribution with a sharp peak at its mean
  / median / mode and fairly long tails. This distribution can be motivated
  as a scale mixture of normal distributions and the remarks above about the
  normal distribution apply here as well.</p>
<p>The lasso approach to supervised learning can be expressed as finding the
  posterior mode when the likelihood is Gaussian and the priors on the
  coefficients have independent Laplace distributions. It is commonplace in
  supervised learning to choose the tuning parameter by cross-validation,
  whereas a more Bayesian approach would be to place a prior on “it”,
  or rather its reciprocal in our case (i.e. <em>smaller</em> values correspond
  to more shrinkage toward the prior location vector). We use a chi-square
  prior with degrees of freedom equal to that specified in the call to
  <code>lasso</code> or, by default, 1. The expectation of a chi-square random
  variable is equal to this degrees of freedom and the mode is equal to the
  degrees of freedom minus 2, if this difference is positive.</p>
<p>It is also common in supervised learning to standardize the predictors
  before training the model. We do not recommend doing so. Instead, it is
  better to specify <code>autoscale = TRUE</code>, which
  will adjust the scales of the priors according to the dispersion in the
  variables. See the documentation of the <code>autoscale</code> argument above
  and also the <code><a href="prior_summary.stanreg.html">prior_summary</a></code> page for more information.</p>
</div>
<div class="section">
<h3 id="product-normal-family">Product-normal family<a class="anchor" aria-label="anchor" href="#product-normal-family"></a></h3>
<p>Family members:</p><ul><li><p><code>product_normal(df, location, scale)</code></p></li>
</ul><p>The product-normal distribution is the product of at least two independent
  normal variates each with mean zero, shifted by the <code>location</code>
  parameter. It can be shown that the density of a product-normal variate is
  symmetric and infinite at <code>location</code>, so this prior resembles a
  “spike-and-slab” prior for sufficiently large values of the
  <code>scale</code> parameter. For better or for worse, this prior may be
  appropriate when it is strongly believed (by someone) that a regression
  coefficient “is” equal to the <code>location</code>, parameter even though
  no true Bayesian would specify such a prior.</p>
<p>Each element of <code>df</code> must be an integer of at least \(2\) because
  these “degrees of freedom” are interpreted as the number of normal
  variates being multiplied and then shifted by <code>location</code> to yield the
  regression coefficient. Higher degrees of freedom produce a sharper
  spike at <code>location</code>.</p>
<p>Each element of <code>scale</code> must be a non-negative real number that is
  interpreted as the standard deviation of the normal variates being
  multiplied and then shifted by <code>location</code> to yield the regression
  coefficient. In other words, the elements of <code>scale</code> may differ, but
  the k-th standard deviation is presumed to hold for all the normal deviates
  that are multiplied together and shifted by the k-th element of
  <code>location</code> to yield the k-th regression coefficient. The elements of
  <code>scale</code> are not the prior standard deviations of the regression
  coefficients. The prior variance of the regression coefficients is equal to
  the scale raised to the power of \(2\) times the corresponding element of
  <code>df</code>. Thus, larger values of <code>scale</code> put more prior volume on
  values of the regression coefficient that are far from zero.</p>
</div>
<div class="section">
<h3 id="dirichlet-family">Dirichlet family<a class="anchor" aria-label="anchor" href="#dirichlet-family"></a></h3>
<p>Family members:</p><ul><li><p><code>dirichlet(concentration)</code></p></li>
</ul><p>The Dirichlet distribution is a multivariate generalization of the beta
  distribution. It is perhaps the easiest prior distribution to specify
  because the concentration parameters can be interpreted as prior counts
  (although they need not be integers) of a multinomial random variable.</p>
<p>The Dirichlet distribution is used in <code><a href="stan_polr.html">stan_polr</a></code> for an
  implicit prior on the cutpoints in an ordinal regression model. More
  specifically, the Dirichlet prior pertains to the prior probability of
  observing each category of the ordinal outcome when the predictors are at
  their sample means. Given these prior probabilities, it is straightforward
  to add them to form cumulative probabilities and then use an inverse CDF
  transformation of the cumulative probabilities to define the cutpoints.</p>
<p>If a scalar is passed to the <code>concentration</code> argument of the
  <code>dirichlet</code> function, then it is replicated to the appropriate length
  and the Dirichlet distribution is symmetric. If <code>concentration</code> is a
  vector and all elements are \(1\), then the Dirichlet distribution is
  jointly uniform. If all concentration parameters are equal but greater than
  \(1\) then the prior mode is that the categories are equiprobable, and
  the larger the value of the identical concentration parameters, the more
  sharply peaked the distribution is at the mode. The elements in
  <code>concentration</code> can also be given different values to represent that
  not all outcome categories are a priori equiprobable.</p>
</div>
<div class="section">
<h3 id="covariance-matrices">Covariance matrices<a class="anchor" aria-label="anchor" href="#covariance-matrices"></a></h3>
<p>Family members:</p><ul><li><p><code>decov(regularization, concentration, shape, scale)</code></p></li>
<li><p><code>lkj(regularization, scale, df)</code></p></li>
</ul><p>(Also see vignette for <code>stan_glmer</code>,
  <a href="https://mc-stan.org/rstanarm/articles/glmer.html"><em>Estimating
  Generalized (Non-)Linear Models with Group-Specific Terms with rstanarm</em></a>)</p>
<p>Covariance matrices are decomposed into correlation matrices and
  variances. The variances are in turn decomposed into the product of a
  simplex vector and the trace of the matrix. Finally, the trace is the
  product of the order of the matrix and the square of a scale parameter.
  This prior on a covariance matrix is represented by the <code>decov</code>
  function.</p>
<p>The prior for a correlation matrix is called LKJ whose density is
  proportional to the determinant of the correlation matrix raised to the
  power of a positive regularization parameter minus one. If
  <code>regularization = 1</code> (the default), then this prior is jointly
  uniform over all correlation matrices of that size. If
  <code>regularization &gt; 1</code>, then the identity matrix is the mode and in the
  unlikely case that <code>regularization &lt; 1</code>, the identity matrix is the
  trough.</p>
<p>The trace of a covariance matrix is equal to the sum of the variances. We
  set the trace equal to the product of the order of the covariance matrix
  and the <em>square</em> of a positive scale parameter. The particular
  variances are set equal to the product of a simplex vector — which is
  non-negative and sums to \(1\) — and the scalar trace. In other words,
  each element of the simplex vector represents the proportion of the trace
  attributable to the corresponding variable.</p>
<p>A symmetric Dirichlet prior is used for the simplex vector, which has a
  single (positive) <code>concentration</code> parameter, which defaults to
  \(1\) and implies that the prior is jointly uniform over the space of
  simplex vectors of that size. If <code>concentration &gt; 1</code>, then the prior
  mode corresponds to all variables having the same (proportion of total)
  variance, which can be used to ensure the the posterior variances are not
  zero. As the <code>concentration</code> parameter approaches infinity, this
  mode becomes more pronounced. In the unlikely case that
  <code>concentration &lt; 1</code>, the variances are more polarized.</p>
<p>If all the variables were multiplied by a number, the trace of their
  covariance matrix would increase by that number squared. Thus, it is
  reasonable to use a scale-invariant prior distribution for the positive
  scale parameter, and in this case we utilize a Gamma distribution, whose
  <code>shape</code> and <code>scale</code> are both \(1\) by default, implying a
  unit-exponential distribution. Set the <code>shape</code> hyperparameter to some
  value greater than \(1\) to ensure that the posterior trace is not zero.</p>
<p>If <code>regularization</code>, <code>concentration</code>, <code>shape</code> and / or
  <code>scale</code> are positive scalars, then they are recycled to the
  appropriate length. Otherwise, each can be a positive vector of the
  appropriate length, but the appropriate length depends on the number of
  covariance matrices in the model and their sizes. A one-by-one covariance
  matrix is just a variance and thus does not have <code>regularization</code> or
  <code>concentration</code> parameters, but does have <code>shape</code> and
  <code>scale</code> parameters for the prior standard deviation of that
  variable.</p>
<p>Note that for <code><a href="stan_mvmer.html">stan_mvmer</a></code> and <code><a href="stan_jm.html">stan_jm</a></code> models an
  additional prior distribution is provided through the <code>lkj</code> function.
  This prior is in fact currently used as the default for those modelling
  functions (although <code>decov</code> is still available as an option if the user
  wishes to specify it through the <code>prior_covariance</code> argument). The
  <code>lkj</code> prior uses the same decomposition of the covariance matrices
  into correlation matrices and variances, however, the variances are not
  further decomposed into a simplex vector and the trace; instead the
  standard deviations (square root of the variances) for each of the group
  specific parameters are given a half Student t distribution with the
  scale and df parameters specified through the <code>scale</code> and <code>df</code>
  arguments to the <code>lkj</code> function. The scale parameter default is 10
  which is then autoscaled, whilst the df parameter default is 1
  (therefore equivalent to a half Cauchy prior distribution for the
  standard deviation of each group specific parameter). This prior generally
  leads to similar results as the <code>decov</code> prior, but it is also likely
  to be **less** diffuse compared with the <code>decov</code> prior; therefore it
  sometimes seems to lead to faster estimation times, hence why it has
  been chosen as the default prior for <code><a href="stan_mvmer.html">stan_mvmer</a></code> and
  <code><a href="stan_jm.html">stan_jm</a></code> where estimation times can be long.</p>
</div>
<div class="section">
<h3 id="r-family">R2 family<a class="anchor" aria-label="anchor" href="#r-family"></a></h3>
<p>Family members:</p><ul><li><p><code>R2(location, what)</code></p></li>
</ul><p>The <code><a href="stan_lm.html">stan_lm</a></code>, <code><a href="stan_lm.html">stan_aov</a></code>, and
  <code><a href="stan_polr.html">stan_polr</a></code> functions allow the user to utilize a function
  called <code>R2</code> to convey prior information about all the parameters.
  This prior hinges on prior beliefs about the location of \(R^2\), the
  proportion of variance in the outcome attributable to the predictors,
  which has a <code><a href="https://rdrr.io/r/stats/Beta.html" class="external-link">Beta</a></code> prior with first shape
  hyperparameter equal to half the number of predictors and second shape
  hyperparameter free. By specifying <code>what</code> to be the prior mode (the
  default), mean, median, or expected log of \(R^2\), the second shape
  parameter for this Beta distribution is determined internally. If
  <code>what = 'log'</code>, location should be a negative scalar; otherwise it
  should be a scalar on the \((0,1)\) interval.</p>
<p>For example, if \(R^2 = 0.5\), then the mode, mean, and median of
  the <code><a href="https://rdrr.io/r/stats/Beta.html" class="external-link">Beta</a></code> distribution are all the same and thus the
  second shape parameter is also equal to half the number of predictors.
  The second shape parameter of the <code><a href="https://rdrr.io/r/stats/Beta.html" class="external-link">Beta</a></code> distribution
  is actually the same as the shape parameter in the LKJ prior for a
  correlation matrix described in the previous subsection. Thus, the smaller
  is \(R^2\), the larger is the shape parameter, the smaller are the
  prior correlations among the outcome and predictor variables, and the more
  concentrated near zero is the prior density for the regression
  coefficients. Hence, the prior on the coefficients is regularizing and
  should yield a posterior distribution with good out-of-sample predictions
  <em>if</em> the prior location of \(R^2\) is specified in a reasonable
  fashion.</p>
</div>

    </div>
    <div class="section level2">
    <h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h2>
    <p>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari,
  A., and Rubin, D. B. (2013). <em>Bayesian Data Analysis.</em> Chapman &amp; Hall/CRC
  Press, London, third edition. <a href="https://stat.columbia.edu/~gelman/book/" class="external-link">https://stat.columbia.edu/~gelman/book/</a></p>
<p>Gelman, A., Jakulin, A., Pittau, M. G., and Su, Y. (2008). A weakly
informative default prior distribution for logistic and other regression
models. <em>Annals of Applied Statistics</em>. 2(4), 1360–1383.</p>
<p>Piironen, J., and Vehtari, A. (2017). Sparsity information and regularization
in the horseshoe and other shrinkage priors. <a href="https://arxiv.org/abs/1707.01694" class="external-link">https://arxiv.org/abs/1707.01694</a></p>
<p>Stan Development Team. <em>Stan Modeling Language Users Guide and
Reference Manual.</em> <a href="https://mc-stan.org/users/documentation/" class="external-link">https://mc-stan.org/users/documentation/</a>.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index"><p>The various vignettes for the <span class="pkg">rstanarm</span> package also discuss
  and demonstrate the use of some of the supported prior distributions.</p></div>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="va">.Platform</span><span class="op">$</span><span class="va">OS.type</span> <span class="op">!=</span> <span class="st">"windows"</span> <span class="op">||</span> <span class="va">.Platform</span><span class="op">$</span><span class="va">r_arch</span> <span class="op">!=</span> <span class="st">"i386"</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span><span class="va">fmla</span> <span class="op">&lt;-</span> <span class="va">mpg</span> <span class="op">~</span> <span class="va">wt</span> <span class="op">+</span> <span class="va">qsec</span> <span class="op">+</span> <span class="va">drat</span> <span class="op">+</span> <span class="va">am</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Draw from prior predictive distribution (by setting prior_PD = TRUE)</span></span></span>
<span class="r-in"><span><span class="va">prior_pred_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="stan_glm.html">stan_glm</a></span><span class="op">(</span><span class="va">fmla</span>, data <span class="op">=</span> <span class="va">mtcars</span>, prior_PD <span class="op">=</span> <span class="cn">TRUE</span>,</span></span>
<span class="r-in"><span>                           chains <span class="op">=</span> <span class="fl">1</span>, seed <span class="op">=</span> <span class="fl">12345</span>, iter <span class="op">=</span> <span class="fl">250</span>, <span class="co"># for speed only</span></span></span>
<span class="r-in"><span>                           prior <span class="op">=</span> <span class="fu">student_t</span><span class="op">(</span>df <span class="op">=</span> <span class="fl">4</span>, <span class="fl">0</span>, <span class="fl">2.5</span><span class="op">)</span>, </span></span>
<span class="r-in"><span>                           prior_intercept <span class="op">=</span> <span class="fu">cauchy</span><span class="op">(</span><span class="fl">0</span>,<span class="fl">10</span><span class="op">)</span>, </span></span>
<span class="r-in"><span>                           prior_aux <span class="op">=</span> <span class="fu">exponential</span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">prior_pred_fit</span>, <span class="st">"hist"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># \donttest{</span></span></span>
<span class="r-in"><span><span class="co"># Can assign priors to names</span></span></span>
<span class="r-in"><span><span class="va">N05</span> <span class="op">&lt;-</span> <span class="fu">normal</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">5</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="stan_glm.html">stan_glm</a></span><span class="op">(</span><span class="va">fmla</span>, data <span class="op">=</span> <span class="va">mtcars</span>, prior <span class="op">=</span> <span class="va">N05</span>, prior_intercept <span class="op">=</span> <span class="va">N05</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Visually compare normal, student_t, cauchy, laplace, and product_normal</span></span></span>
<span class="r-in"><span><span class="va">compare_priors</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">scale</span> <span class="op">=</span> <span class="fl">1</span>, <span class="va">df_t</span> <span class="op">=</span> <span class="fl">2</span>, <span class="va">xlim</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">10</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>  <span class="va">dt_loc_scale</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">df</span>, <span class="va">location</span>, <span class="va">scale</span><span class="op">)</span> <span class="op">{</span> </span></span>
<span class="r-in"><span>    <span class="fl">1</span><span class="op">/</span><span class="va">scale</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html" class="external-link">dt</a></span><span class="op">(</span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">location</span><span class="op">)</span><span class="op">/</span><span class="va">scale</span>, <span class="va">df</span><span class="op">)</span>  </span></span>
<span class="r-in"><span>  <span class="op">}</span></span></span>
<span class="r-in"><span>  <span class="va">dlaplace</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">location</span>, <span class="va">scale</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>    <span class="fl">0.5</span> <span class="op">/</span> <span class="va">scale</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">location</span><span class="op">)</span> <span class="op">/</span> <span class="va">scale</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="op">}</span></span></span>
<span class="r-in"><span>  <span class="va">dproduct_normal</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">scale</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>    <span class="fu"><a href="https://rdrr.io/r/base/Bessel.html" class="external-link">besselK</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">/</span> <span class="va">scale</span> <span class="op">^</span> <span class="fl">2</span>, nu <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">scale</span> <span class="op">^</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="op">}</span></span></span>
<span class="r-in"><span>  <span class="va">stat_dist</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">dist</span>, <span class="va">...</span><span class="op">)</span> <span class="op">{</span></span></span>
<span class="r-in"><span>    <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html" class="external-link">stat_function</a></span><span class="op">(</span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes_.html" class="external-link">aes_</a></span><span class="op">(</span>color <span class="op">=</span> <span class="va">dist</span><span class="op">)</span>, <span class="va">...</span><span class="op">)</span></span></span>
<span class="r-in"><span>  <span class="op">}</span></span></span>
<span class="r-in"><span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">xlim</span><span class="op">)</span>, <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span></span>
<span class="r-in"><span>    <span class="fu">stat_dist</span><span class="op">(</span><span class="st">"normal"</span>, size <span class="op">=</span> <span class="fl">.75</span>, fun <span class="op">=</span> <span class="va">dnorm</span>, </span></span>
<span class="r-in"><span>              args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="va">scale</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span></span>
<span class="r-in"><span>    <span class="fu">stat_dist</span><span class="op">(</span><span class="st">"student_t"</span>, size <span class="op">=</span> <span class="fl">.75</span>, fun <span class="op">=</span> <span class="va">dt_loc_scale</span>, </span></span>
<span class="r-in"><span>              args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>df <span class="op">=</span> <span class="va">df_t</span>, location <span class="op">=</span> <span class="fl">0</span>, scale <span class="op">=</span> <span class="va">scale</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span></span>
<span class="r-in"><span>    <span class="fu">stat_dist</span><span class="op">(</span><span class="st">"cauchy"</span>, size <span class="op">=</span> <span class="fl">.75</span>, linetype <span class="op">=</span> <span class="fl">2</span>, fun <span class="op">=</span> <span class="va">dcauchy</span>, </span></span>
<span class="r-in"><span>              args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>location <span class="op">=</span> <span class="fl">0</span>, scale <span class="op">=</span> <span class="va">scale</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span></span>
<span class="r-in"><span>    <span class="fu">stat_dist</span><span class="op">(</span><span class="st">"laplace"</span>, size <span class="op">=</span> <span class="fl">.75</span>, linetype <span class="op">=</span> <span class="fl">2</span>, fun <span class="op">=</span> <span class="va">dlaplace</span>,</span></span>
<span class="r-in"><span>              args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>location <span class="op">=</span> <span class="fl">0</span>, scale <span class="op">=</span> <span class="va">scale</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span></span>
<span class="r-in"><span>    <span class="fu">stat_dist</span><span class="op">(</span><span class="st">"product_normal"</span>, size <span class="op">=</span> <span class="fl">.75</span>, linetype <span class="op">=</span> <span class="fl">2</span>, fun <span class="op">=</span> <span class="va">dproduct_normal</span>,</span></span>
<span class="r-in"><span>              args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>scale <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span>            </span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-in"><span><span class="co"># Cauchy has fattest tails, followed by student_t, laplace, and normal</span></span></span>
<span class="r-in"><span><span class="fu">compare_priors</span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># The student_t with df = 1 is the same as the cauchy</span></span></span>
<span class="r-in"><span><span class="fu">compare_priors</span><span class="op">(</span>df_t <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> </span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Even a scale of 5 is somewhat large. It gives plausibility to rather </span></span></span>
<span class="r-in"><span><span class="co"># extreme values</span></span></span>
<span class="r-in"><span><span class="fu">compare_priors</span><span class="op">(</span>scale <span class="op">=</span> <span class="fl">5</span>, xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">20</span>,<span class="fl">20</span><span class="op">)</span><span class="op">)</span> </span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># If you use a prior like normal(0, 1000) to be "non-informative" you are </span></span></span>
<span class="r-in"><span><span class="co"># actually saying that a coefficient value of e.g. -500 is quite plausible</span></span></span>
<span class="r-in"><span><span class="fu">compare_priors</span><span class="op">(</span>scale <span class="op">=</span> <span class="fl">1000</span>, xlim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1000</span>,<span class="fl">1000</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Gradient evaluation took 2e-05 seconds</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Adjust your expectations accordingly!</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: WARNING: There aren't enough warmup iterations to fit the</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1:          three stages of adaptation as currently configured.</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1:          Reducing each adaptation stage to 15%/75%/10% of</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1:          the given number of warmup iterations:</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1:            init_buffer = 18</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1:            adapt_window = 95</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1:            term_buffer = 12</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration:   1 / 250 [  0%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration:  25 / 250 [ 10%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration:  50 / 250 [ 20%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration:  75 / 250 [ 30%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 100 / 250 [ 40%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 125 / 250 [ 50%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 126 / 250 [ 50%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 150 / 250 [ 60%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 175 / 250 [ 70%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 200 / 250 [ 80%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 225 / 250 [ 90%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 250 / 250 [100%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1:  Elapsed Time: 0.015 seconds (Warm-up)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1:                0.016 seconds (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1:                0.031 seconds (Total)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: </span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>The largest R-hat is 1.14, indicating chains have not mixed.</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> Running the chains for more iterations may help. See</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> https://mc-stan.org/misc/warnings.html#r-hat</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> Running the chains for more iterations may help. See</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> https://mc-stan.org/misc/warnings.html#bulk-ess</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> Running the chains for more iterations may help. See</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> https://mc-stan.org/misc/warnings.html#tail-ess</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Gradient evaluation took 2e-05 seconds</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Adjust your expectations accordingly!</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1:  Elapsed Time: 0.05 seconds (Warm-up)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1:                0.048 seconds (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1:                0.098 seconds (Total)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 1: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Gradient evaluation took 1.1e-05 seconds</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Adjust your expectations accordingly!</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2:  Elapsed Time: 0.051 seconds (Warm-up)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2:                0.052 seconds (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2:                0.103 seconds (Total)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 2: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Gradient evaluation took 1.1e-05 seconds</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Adjust your expectations accordingly!</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3:  Elapsed Time: 0.051 seconds (Warm-up)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3:                0.064 seconds (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3:                0.115 seconds (Total)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 3: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Gradient evaluation took 1.1e-05 seconds</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Adjust your expectations accordingly!</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: </span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4:  Elapsed Time: 0.054 seconds (Warm-up)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4:                0.052 seconds (Sampling)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4:                0.106 seconds (Total)</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> Chain 4: </span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span class="warning">Warning: </span>`aes_()` was deprecated in ggplot2 3.0.0.</span>
<span class="r-wrn co"><span class="r-pr">#&gt;</span> <span style="color: #00BBBB;">ℹ</span> Please use tidy evaluation idioms with `aes()`</span>
<span class="r-plt img"><img src="priors-1.png" alt="" width="700" height="433"></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Jonah Gabry, Ben Goodrich.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer></div>





  </body></html>

